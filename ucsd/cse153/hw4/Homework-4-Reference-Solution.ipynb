{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to HW4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load in our model, and initialize our global variables of SAMPLE_RATE (i.e. the samples per second of the audio, in this case 44100), SAMPLE_SIZE (the *number* of audio samples we generate with the model, approximately 47.55*44100), and SEED (controls randomness, DO NOT CHANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio              # For audio-specific operations\n",
    "from einops import rearrange   # Easy tensor reshaping\n",
    "from stable_audio_tools import get_pretrained_model  # Load pretrained Stable Audio model\n",
    "import IPython.display as ipd  # For playing audio in notebooks\n",
    "from tqdm.auto import trange, tqdm  # Progress bars for loops\n",
    "from stable_audio_tools.inference.generation import (\n",
    "    generate_diffusion_cond_and_sampler_setup,\n",
    "    generate_diffusion_cond_decode\n",
    ")\n",
    "import gc  # Garbage collection (helps manage memory in big models)\n",
    "\n",
    "# This is to choose device: GPU if available, otherwise CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load pretrained diffusion model + config info (e.g., sample rate)\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-1.0\")\n",
    "SAMPLE_RATE = model_config[\"sample_rate\"]             # Samples per second (e.g. 44100 Hz)\n",
    "SAMPLE_SIZE = model_config[\"sample_size\"] // 8       # Total samples per clip (downscaled for latent)\n",
    "SEED = 456                                            # For reproducibility, this controls randomness\n",
    "\n",
    "# Moves model to your device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using collab, uncomment the following lines\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('[/content/drive]')\n",
    "# cd /content/drive/MyDrive/[path to your folder]\n",
    "# pip install -e .\n",
    "# pip install numpy==1.26.4\n",
    "# pip install protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Simple Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should implement the to_d and simple_sample functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Q1: To Convert model output (x-prediction) to derivative (gradient direction) ===\n",
    "\n",
    "def to_d(x, sigma, denoised):\n",
    "    # SAO predicts the denoised x₀ directly (x-prediction), not the noise.\n",
    "    # To get the derivative (dx/dt), we use: derivative = (x - x₀) / sigma\n",
    "    return (x - denoised) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # We don't need gradients during inference (an optimization technique)\n",
    "def simple_sample(model, x, sigmas, extra_args=None):\n",
    "    \n",
    "    extra_args = {} if extra_args is None else extra_args\n",
    "    \n",
    "    # s_in is a scale tensor needed by SAO (must be a tensor, not float)\n",
    "    s_in = x.new_ones([x.shape[0]])\n",
    "    \n",
    "    for i in trange(len(sigmas) - 1):\n",
    "        # === Step 1: Denoise current x ===\n",
    "        # Model expects (x, sigmas[i], **kwargs) format\n",
    "        denoised = model(x, sigmas[i] * s_in, **extra_args)\n",
    "        \n",
    "        # === Step 2: Estimate the derivative ===\n",
    "        d = to_d(x, sigmas[i], denoised)\n",
    "\n",
    "        # === Step 3: Compute step size (Euler integration) ===\n",
    "        dt = sigmas[i + 1] - sigmas[i]\n",
    "        \n",
    "        # === Step 4: Take a step in the reverse diffusion process ===\n",
    "        x = x + d * dt\n",
    "\n",
    "    # Clean up memory after sampling\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given your code, you can now run it using this below block. Feel free to play around with the prompt in the conditioning list, the number of steps, and cfg_scale to explore unique outputs. This can help you test your code, as if it sounds bad, you're probably doing something wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt=\"128 BPM electronic drum loop\", steps=50, cfg_scale=7, return_latents=False):\n",
    "\n",
    "    # Set up text and timing conditioning\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 5\n",
    "    }]\n",
    "\n",
    "    # Generate diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps, # number of steps, more = better quality\n",
    "        cfg_scale=cfg_scale, # Classifier-Free Guidance Scale, higher = better text relevance / quality but less diversity\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE, # number of audio samples to generate, DON'T CHANGE\n",
    "        device=device, # cuda device\n",
    "        seed=SEED # random seed, DON'T CHANGE\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    samples = simple_sample(denoiser, x_T, sigmas, extra_args=extra_args)\n",
    "    del x_T\n",
    "    del sigmas\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if return_latents:\n",
    "        return samples\n",
    "\n",
    "    # Decode\n",
    "    audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        samples\n",
    "    ).cpu()\n",
    "    return audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: Run and listen to generation results (GPU recommended) ===\n",
    "# NOTE: Running on CPU is very slow and not required for this assignment.\n",
    "\n",
    "for ix, prompt in enumerate([\n",
    "    \"lo-fi jazz piano in a rainy cafe\",\n",
    "    \"deep ambient wash with ocean sounds\"\n",
    "]):\n",
    "    # === Generate audio from prompt using simple sampler ===\n",
    "    audio = generate(\n",
    "        prompt=prompt,\n",
    "        steps=50,\n",
    "        cfg_scale=7,\n",
    "        return_latents=False\n",
    "    )\n",
    "\n",
    "    # === Play the generated audio ===\n",
    "    ipd.display(ipd.Audio(audio.cpu().numpy()[0], rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Summary – What are we doing?\n",
    "\n",
    "In this question, we are implementing the **core sampler** for generating audio using diffusion.\n",
    "\n",
    "- `to_d`: Converts the model's prediction (denoised audio) into a \"score\", the gradient direction that helps us denoise.\n",
    "- `simple_sample`: The main loop that starts with noise and walks it backward using the model, step-by-step.\n",
    "- The model uses **x-prediction**, so we have to convert it to the time-derivative it using \\[(x - denoised) / sigma\\].\n",
    "- We use a basic numerical solver (Euler method) to go from pure noise to a clean sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Inpainting Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD AND ENCODE REFERENCE AUDIO ===\n",
    "def load_and_encode_audio(path, model):\n",
    "    audio, sr = torchaudio.load(path)\n",
    "    \n",
    "    # === Resample audio to model's expected SAMPLE_RATE ===\n",
    "    resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
    "    audio = resampler(audio)\n",
    "\n",
    "    # === Normalize audio to [-1, 1] by peak value ===\n",
    "    audio = audio / audio.abs().max()\n",
    "\n",
    "    # === Ensure audio length matches SAMPLE_SIZE ===\n",
    "    # If too short, tile the audio; if too long, crop\n",
    "    if audio.shape[1] < SAMPLE_SIZE:\n",
    "        while audio.shape[1] < SAMPLE_SIZE:\n",
    "            audio = torch.cat((audio, audio), dim=1)\n",
    "\n",
    "    audio = audio[:, :SAMPLE_SIZE][None].to(device)\n",
    "\n",
    "    # === Encode into latent space (for inpainting) ===\n",
    "    reference = model.pretransform.encode(audio)\n",
    "    return reference\n",
    "\n",
    "\n",
    "def load_encoded_audio(path):\n",
    "    encoded_latent = torch.load(path)\n",
    "    # Move to device and convert to float16\n",
    "    return encoded_latent.to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inpainting_mask(reference, mask_start_s, mask_end_s):\n",
    "    # Convert mask start/end times in seconds to latent indices\n",
    "    mask_start = int(mask_start_s * SAMPLE_RATE // model.pretransform.downsampling_ratio)\n",
    "    mask_end = int(mask_end_s * SAMPLE_RATE // model.pretransform.downsampling_ratio)\n",
    "\n",
    "    # Create a zero mask of the same shape as the reference latent\n",
    "    mask = torch.zeros_like(reference)\n",
    "\n",
    "    # Set mask = 1 in the region to be inpainted\n",
    "    mask[..., mask_start:mask_end] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Summary – What’s going on here?\n",
    "\n",
    "We are preparing reference audio for **inpainting**, where we \"fill in\" a missing segment using a diffusion model.\n",
    "\n",
    "- First, we **load and normalize the reference audio**.\n",
    "- Then, we **encode it into latent space**, since all our operations are done on latents (to save compute).\n",
    "- Next, we create an **inpainting mask**:\n",
    "  - The mask has `1`s where we want the model to generate new content.\n",
    "  - And `0`s where we want to keep the original audio untouched.\n",
    "  - The time ranges are converted from seconds → latent indices using the sample rate and downsampling ratio.\n",
    "\n",
    "We'll use this mask in the next question to guide the generation process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3  - Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simple_sample_inpaint(model, x, sigmas, reference, mask, extra_args=None):\n",
    "    \"\"\"Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\"\"\"\n",
    "    extra_args = {} if extra_args is None else extra_args\n",
    "    s_in = x.new_ones([x.shape[0]])  # Needed for classifier-free guidance scale\n",
    "\n",
    "    for i in trange(len(sigmas) - 1):\n",
    "        # === 1. Denoise current sample\n",
    "        denoised = model(x, sigmas[i] * s_in, **extra_args)\n",
    "\n",
    "        # === 2. Convert model output to derivative\n",
    "        d = to_d(x, sigmas[i], denoised)\n",
    "\n",
    "        # === 3. Euler step update\n",
    "        dt = sigmas[i + 1] - sigmas[i]\n",
    "        x = x + d * dt\n",
    "\n",
    "        # === 4. Inpainting step\n",
    "        # Add noise to reference to match current noise level\n",
    "        ref = reference + torch.randn_like(reference) * sigmas[i + 1]\n",
    "\n",
    "        # Replace unmasked regions in x with noisy reference\n",
    "        x = x * mask + (1 - mask) * ref\n",
    "\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint(prompt=\"128 BPM house drum loop\", steps=50, cfg_scale=7,\n",
    "            reference=None, mask_start_s=2.0, mask_end_s=3.0,\n",
    "            return_latents=False):\n",
    "\n",
    "    # === 1. Text + timing conditioning ===\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0,\n",
    "        \"seconds_total\": 5\n",
    "    }]\n",
    "\n",
    "    # === 2. Generate binary mask for inpainting\n",
    "    mask = generate_inpainting_mask(reference, mask_start_s, mask_end_s)\n",
    "\n",
    "    # === 3. Setup denoiser and diffusion parameters\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # === 4. Run inpainting-aware sampler\n",
    "    inp_samples = simple_sample_inpaint(denoiser, x_T, sigmas, reference, mask, extra_args=extra_args)\n",
    "\n",
    "    # Clean up unused memory\n",
    "    del x_T, sigmas, extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Optional return: raw latents\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "\n",
    "    # === 5. Decode latents to audio\n",
    "    inpainted_audio = generate_diffusion_cond_decode(model, inp_samples).cpu()\n",
    "    return inpainted_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: Run and listen to inpainting results (GPU recommended) ===\n",
    "# This is slow on CPU and not required for submission.\n",
    "\n",
    "for ix, prompt in enumerate([\n",
    "    \"lo-fi jazz piano in a rainy cafe\",\n",
    "    \"deep ambient wash with ocean sounds\"\n",
    "]):\n",
    "    # === Load reference latent from file ===\n",
    "    reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")\n",
    "\n",
    "    # === Define inpainting mask (in seconds) ===\n",
    "    mask = generate_inpainting_mask(reference, mask_start_s=0, mask_end_s=3)\n",
    "\n",
    "    # === Perform inpainting using mask and prompt ===\n",
    "    audio = inpaint(\n",
    "        prompt=prompt,\n",
    "        steps=50,\n",
    "        cfg_scale=7,\n",
    "        reference=reference,\n",
    "        mask_start_s=0,\n",
    "        mask_end_s=3,\n",
    "        return_latents=False\n",
    "    )\n",
    "\n",
    "    # === Listen to the output ===\n",
    "    ipd.display(ipd.Audio(audio.cpu().numpy()[0], rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Summary – What’s happening?\n",
    "\n",
    "This is your first true “guided generation” task where you’ll fill in only a **masked segment** of audio while preserving the rest.\n",
    "\n",
    "Key logic:\n",
    "- Add noise to the reference latent to match the current noise level.\n",
    "- Use the binary mask to **only update the masked region**.\n",
    "- Outside the mask, we just reuse the noisy reference every step.\n",
    "\n",
    "This allows creative audio editing with full control over what gets changed and what stays!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Painting with Starting and Stopping Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simple_sample_variable_inpaint(model, x, sigmas, reference, mask, extra_args=None, paint_start=None, paint_end=None):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\n",
    "    Diffusion sampler with selective inpainting steps.\n",
    "    Inpaints only during [paint_start, paint_end] timesteps.\n",
    "    \"\"\"\n",
    "    # Default to full range if not specified\n",
    "    if paint_start is None:\n",
    "        paint_start = 0\n",
    "    if paint_end is None:\n",
    "        paint_end = len(sigmas) - 1\n",
    "\n",
    "    extra_args = {} if extra_args is None else extra_args\n",
    "    s_in = x.new_ones([x.shape[0]])\n",
    "\n",
    "    for i in trange(len(sigmas) - 1):\n",
    "        # === 1. Denoise\n",
    "        denoised = model(x, sigmas[i] * s_in, **extra_args)\n",
    "\n",
    "        # === 2. Convert to derivative\n",
    "        d = to_d(x, sigmas[i], denoised)\n",
    "\n",
    "        # === 3. Euler step\n",
    "        dt = sigmas[i + 1] - sigmas[i]\n",
    "        x = x + d * dt\n",
    "\n",
    "        # === 4. Apply inpainting ONLY during chosen step range\n",
    "        if paint_start <= i <= paint_end:\n",
    "            # Add matching noise to reference\n",
    "            ref = reference + torch.randn_like(reference) * sigmas[i + 1]\n",
    "            x = x * mask + (1 - mask) * ref\n",
    "\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_inpaint(prompt=\"128 BPM house drum loop\", steps=50, cfg_scale=7, reference=None, mask_start_s=20, mask_end_s=30, paint_start=None, paint_end=None, return_latents=False):\n",
    "    # === 1. Prompt + timing conditioning ===\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0, \n",
    "        \"seconds_total\": 5\n",
    "    }]\n",
    "    # === 2. Create inpainting mask\n",
    "    mask = generate_inpainting_mask(reference, mask_start_s, mask_end_s)\n",
    "\n",
    "    # === 3. Prepare diffusion setup params\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # === 4. Sample with inpainting only between [paint_start, paint_end] steps\n",
    "    inp_samples = simple_sample_variable_inpaint(denoiser, x_T, sigmas, reference, mask, extra_args=extra_args, paint_start=paint_start, paint_end=paint_end)\n",
    "    del x_T\n",
    "    del sigmas\n",
    "    del extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "\n",
    "    # === 5. Decode to audio\n",
    "    inpainted_audio = generate_diffusion_cond_decode(\n",
    "        model,\n",
    "        inp_samples\n",
    "    ).cpu()\n",
    "    return inpainted_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: Run full audio inpainting if you have a GPU ===\n",
    "# NOTE: This is slow on CPU and NOT required for the homework.\n",
    "# This is just for listening to results.\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "for ix, prompt in enumerate([\n",
    "    \"lo-fi jazz piano in a rainy cafe\",\n",
    "    \"deep ambient wash with ocean sounds\"\n",
    "]):\n",
    "    reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")   # Load latent reference\n",
    "    mask = generate_inpainting_mask(reference, 0, 3)              # Create inpainting mask\n",
    "\n",
    "    if ix == 0:\n",
    "        paint_start = 0\n",
    "        paint_end = 20\n",
    "    else:\n",
    "        paint_start = 15\n",
    "        paint_end = 45\n",
    "\n",
    "    # Run variable-strength inpainting\n",
    "    audio = variable_inpaint(\n",
    "        prompt=prompt,\n",
    "        steps=50,\n",
    "        cfg_scale=7,\n",
    "        reference=reference,\n",
    "        mask_start_s=0,\n",
    "        mask_end_s=3,\n",
    "        paint_start=paint_start,\n",
    "        paint_end=paint_end\n",
    "    )\n",
    "\n",
    "    # Playback\n",
    "    display(Audio(audio.cpu().numpy()[0], rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Summary – What’s happening?\n",
    "\n",
    "This question adds more control by letting you choose **when** inpainting happens during the diffusion process.\n",
    "\n",
    "Key logic:\n",
    "- Only apply inpainting during specific timesteps using `paint_start` and `paint_end`.\n",
    "- This limits how much the model alters the reference, especially near the boundaries.\n",
    "- Outside this step range, the sample evolves normally.\n",
    "\n",
    "Useful for achieving smoother transitions and less aggressive edits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sample_style_transfer(model, sigmas, reference, extra_args=None, transfer_strength=0):\n",
    "    # === Convert transfer strength (0.0–1.0) to a step index ===\n",
    "    inv_step = int(transfer_strength * len(sigmas))\n",
    "\n",
    "    # === Add noise to reference to simulate partial destruction ===\n",
    "    x_t = reference + torch.randn_like(reference) * sigmas[inv_step]\n",
    "\n",
    "    # === Run normal sampling starting from that noisy reference ===\n",
    "    return simple_sample(model, x_t, sigmas[inv_step:], extra_args=extra_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(prompt=\"128 BPM house drum loop\", steps=50, cfg_scale=7, reference=None, transfer_strength=0, return_latents=False):\n",
    "    # === Set up prompt and duration ===\n",
    "    conditioning = [{\n",
    "        \"prompt\": prompt,\n",
    "        \"seconds_start\": 0,\n",
    "        \"seconds_total\": 5\n",
    "    }]\n",
    "\n",
    "    # === Get initial noise, denoiser, sigmas, etc. ===\n",
    "    denoiser, x_T, sigmas, extra_args = generate_diffusion_cond_and_sampler_setup(\n",
    "        model,\n",
    "        steps=steps,\n",
    "        cfg_scale=cfg_scale,\n",
    "        conditioning=conditioning,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        device=device,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # === Run sampling using style-transferred starting point ===\n",
    "    inp_samples = simple_sample_style_transfer(\n",
    "        denoiser, sigmas, reference,\n",
    "        extra_args=extra_args,\n",
    "        transfer_strength=transfer_strength\n",
    "    )\n",
    "\n",
    "    # === Clean up unused memory ===\n",
    "    del x_T, sigmas, extra_args\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if return_latents:\n",
    "        return inp_samples\n",
    "\n",
    "    # === Decode to waveform ===\n",
    "    inpainted_audio = generate_diffusion_cond_decode(model, inp_samples).cpu()\n",
    "    return inpainted_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === For those running with a GPU, you can also generate and listen to audio samples ===\n",
    "# This is NOT recommended for CPU-only users due to long runtimes and memory requirements.\n",
    "\n",
    "# Loop over two style prompts\n",
    "# Each prompt has a different reference audio and transfer strength\n",
    "for ix, prompt in enumerate([\n",
    "    \"deep ambient wash with ocean sounds\"\n",
    "    \"lo-fi jazz piano in a rainy cafe\",\n",
    "]):\n",
    "    # === Load encoded latent reference ===\n",
    "    # These are pre-encoded .pt files with reference style audio in latent space\n",
    "    reference = load_encoded_audio(f\"testing_files/q1_{ix}.pt\")\n",
    "\n",
    "    # === Define how much of the reference style to inject ===\n",
    "    # 0.0 = no influence from reference (pure prompt)\n",
    "    # 1.0 = fully follow reference audio, less prompt effect\n",
    "    if ix == 0:\n",
    "        transfer_strength = 0.2  # Mostly prompt-driven\n",
    "    else:\n",
    "        transfer_strength = 0.5  # More reference-style preserved\n",
    "\n",
    "    # === Generate audio with style transfer ===\n",
    "    audio = style_transfer(\n",
    "        prompt=prompt,\n",
    "        steps=50,\n",
    "        cfg_scale=7,\n",
    "        reference=reference,\n",
    "        transfer_strength=transfer_strength,\n",
    "        return_latents=False\n",
    "    )\n",
    "\n",
    "    # === Listen to the result directly in notebook ===\n",
    "    ipd.display(ipd.Audio(audio.cpu().numpy()[0], rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Summary – What’s happening?\n",
    "\n",
    "This task performs style transfer by combining a reference audio with a text prompt.\n",
    "\n",
    "Key logic:\n",
    "- Add noise to the reference latent up to a certain level based on `transfer_strength`.\n",
    "- Use that noisy latent as the starting point for generation.\n",
    "- Guide the rest of the process using the prompt and the diffusion model.\n",
    "\n",
    "Higher transfer strength keeps more of the reference’s characteristics, while lower values let the prompt have more influence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
