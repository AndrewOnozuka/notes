Task 1: I extracted 177 symbolic features per MIDI file, including pitch range, average duration, note density, pitch class histogram, interval histogram, and a 144-bin interval bigram histogram. I also extracted the key signature and tempo using `music21`, and encoded chord progression with a TF-IDF vectorizer over chord strings. I trained a HistGradientBoostingClassifier (max_iter=300, learning_rate=0.05, max_leaf_nodes=64) on concatenated symbolic + TF-IDF features. The model achieved ~0.97 training accuracy, ~0.85 validation accuracy, and a leaderboard score of ~0.63.

Task 2: For each MIDI file, I extracted the same symbolic features as in Task 1, and added interval histograms, rhythm statistics (mean, std, longest-duration ratio), and key signature info. For each pair, I computed 8 symmetric combinations (concat, diff, product, min/max, ratios). I trained a LightGBM classifier (400 trees, learning_rate=0.03, num_leaves=128) on these combined vectors. This achieved a training accuracy of ~99.8% and a leaderboard score of ~0.85.

Task 3: I used the baseline CNN architecture with dropout increased to 0.3, learning rate set to 3e-4, and trained for 10 epochs. The model achieved a training mAP of ~0.98, a best validation mAP of ~0.66, and a public leaderboard score of ~0.38.