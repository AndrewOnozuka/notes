{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7005d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably more imports than are really necessary...\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import random\n",
    "from music21 import converter, chord\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48503b3f",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255b620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy1(groundtruth, predictions):\n",
    "    correct = 0\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        if predictions[k] == groundtruth[k]:\n",
    "            correct += 1\n",
    "    return correct / len(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56e40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy2(groundtruth, predictions):\n",
    "    correct = 0\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        if predictions[k] == groundtruth[k]:\n",
    "            correct += 1\n",
    "    return correct / len(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f190f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = ['rock', 'oldies', 'jazz', 'pop', 'dance',  'blues',  'punk', 'chill', 'electronic', 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b772218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy3(groundtruth, predictions):\n",
    "    preds, targets = [], []\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        prediction = [1 if tag in predictions[k] else 0 for tag in TAGS]\n",
    "        target = [1 if tag in groundtruth[k] else 0 for tag in TAGS]\n",
    "        preds.append(prediction)\n",
    "        targets.append(target)\n",
    "    \n",
    "    mAP = average_precision_score(targets, preds, average='macro')\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36bf2cf",
   "metadata": {},
   "source": [
    "## Task 3: Audio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb1e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants (you can change any of these if useful)\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 64\n",
    "N_CLASSES = 10\n",
    "AUDIO_DURATION = 10 # seconds\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# %%\n",
    "dataroot3 = \"student_files/task3_audio_classification/\"\n",
    "\n",
    "def extract_waveform(path):\n",
    "    waveform, sr = librosa.load(dataroot3 + '/' + path, sr=SAMPLE_RATE)\n",
    "    waveform = np.array([waveform])\n",
    "    if sr != SAMPLE_RATE:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "        waveform = resample(waveform)\n",
    "    # Pad so that everything is the right length\n",
    "    target_len = SAMPLE_RATE * AUDIO_DURATION\n",
    "    if waveform.shape[1] < target_len:\n",
    "        pad_len = target_len - waveform.shape[1]\n",
    "        waveform = F.pad(waveform, (0, pad_len))\n",
    "    else:\n",
    "        waveform = waveform[:, :target_len]\n",
    "    waveform = torch.FloatTensor(waveform)\n",
    "    return waveform\n",
    "\n",
    "# %%\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, meta, preload = True):\n",
    "        self.meta = meta\n",
    "        ks = list(meta.keys())\n",
    "        self.idToPath = dict(zip(range(len(ks)), ks))\n",
    "        self.pathToFeat = {}\n",
    "\n",
    "        self.mel = MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=N_MELS)\n",
    "        self.db = AmplitudeToDB()\n",
    "        \n",
    "        self.preload = preload # Determines whether the features should be preloaded (uses more memory)\n",
    "                               # or read from disk / computed each time (slow if your system is i/o-bound)\n",
    "        if self.preload:\n",
    "            for path in ks:\n",
    "                waveform = extract_waveform(path)\n",
    "                mel_spec = self.db(self.mel(waveform)).squeeze(0)\n",
    "                self.pathToFeat[path] = mel_spec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Faster version, preloads the features\n",
    "        path = self.idToPath[idx]\n",
    "        tags = self.meta[path]\n",
    "        bin_label = torch.tensor([1 if tag in tags else 0 for tag in TAGS], dtype=torch.float32)\n",
    "\n",
    "        if self.preload:\n",
    "            mel_spec = self.pathToFeat[path]\n",
    "        else:\n",
    "            waveform = extract_waveform(path)\n",
    "            mel_spec = self.db(self.mel(waveform)).squeeze(0)\n",
    "        \n",
    "        return mel_spec.unsqueeze(0), bin_label, path\n",
    "\n",
    "# %%\n",
    "class Loaders():\n",
    "    def __init__(self, train_path, test_path, split_ratio=0.9, seed = 0):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        meta_train = eval(open(train_path, 'r').read())\n",
    "        l_test = eval(open(test_path, 'r').read())\n",
    "        meta_test = dict([(x,[]) for x in l_test]) # Need a dictionary for the above class\n",
    "        \n",
    "        all_train = AudioDataset(meta_train)\n",
    "        test_set = AudioDataset(meta_test)\n",
    "        \n",
    "        # Split all_train into train + valid\n",
    "        total_len = len(all_train)\n",
    "        train_len = int(total_len * split_ratio)\n",
    "        valid_len = total_len - train_len\n",
    "        train_set, valid_set = random_split(all_train, [train_len, valid_len])\n",
    "        \n",
    "        self.loaderTrain = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderValid = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderTest = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# %%\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=N_CLASSES):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(32 * (N_MELS // 4) * (801 // 4), 256)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (B, 16, mel/2, time/2)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (B, 32, mel/4, time/4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return torch.sigmoid(self.fc2(x))  # multilabel â†’ sigmoid\n",
    "\n",
    "# %%\n",
    "class Pipeline():\n",
    "    def __init__(self, model, learning_rate, seed = 0):\n",
    "        # These two lines will (mostly) make things deterministic.\n",
    "        # You're welcome to modify them to try to get a better solution.\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        self.device = torch.device(\"cpu\") # Can change this if you have a GPU, but the autograder will use CPU\n",
    "        self.model = model.to(self.device) #model.cuda() # Also uncomment these lines for GPU\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=1e-5)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def evaluate(self, loader, threshold=0.5, outpath=None):\n",
    "        self.model.eval()\n",
    "        preds, targets, paths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, ps in loader:\n",
    "                x = x.to(self.device) #x.cuda()\n",
    "                y = y.to(self.device) #y.cuda()\n",
    "                outputs = self.model(x)\n",
    "                preds.append(outputs.cpu())\n",
    "                targets.append(y.cpu())\n",
    "                paths += list(ps)\n",
    "        \n",
    "        preds = torch.cat(preds)\n",
    "        targets = torch.cat(targets)\n",
    "        preds_bin = (preds > threshold).float()\n",
    "        \n",
    "        predictions = {}\n",
    "        for i in range(preds_bin.shape[0]):\n",
    "            predictions[paths[i]] = [TAGS[j] for j in range(len(preds_bin[i])) if preds_bin[i][j]]\n",
    "        \n",
    "        mAP = None\n",
    "        if outpath: # Save predictions\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "        else: # Only compute accuracy if we're *not* saving predictions, since we can't compute test accuracy\n",
    "            mAP = average_precision_score(targets, preds, average='macro')\n",
    "        return predictions, mAP\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs=20, patience=3):\n",
    "        best_map = 0\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for x, y, path in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(x)\n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            _, val_mAP = self.evaluate(val_loader)\n",
    "            print(f\"[Epoch {epoch+1}] Loss: {running_loss/len(train_loader):.4f} | Val mAP: {val_mAP:.4f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_mAP > best_map:\n",
    "                best_map = val_mAP\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87197d0",
   "metadata": {},
   "source": [
    "## Run everything..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dbe7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run3():\n",
    "    loaders = Loaders(dataroot3 + \"/train.json\", dataroot3 + \"/test.json\")\n",
    "    model = CNNClassifier()\n",
    "    pipeline = Pipeline(model, learning_rate=3e-4)\n",
    "    \n",
    "    pipeline.train(loaders.loaderTrain, loaders.loaderValid, num_epochs=20, patience=3)\n",
    "    train_preds, train_mAP = pipeline.evaluate(loaders.loaderTrain, 0.5)\n",
    "    valid_preds, valid_mAP = pipeline.evaluate(loaders.loaderValid, 0.5)\n",
    "    test_preds, _ = pipeline.evaluate(loaders.loaderTest, 0.5, \"predictions3.json\")\n",
    "    \n",
    "    all_train = eval(open(dataroot3 + \"/train.json\").read())\n",
    "    for k in valid_preds:\n",
    "        # We split our training set into train+valid\n",
    "        # so need to remove validation instances from the training set for evaluation\n",
    "        all_train.pop(k)\n",
    "    acc3 = accuracy3(all_train, train_preds)\n",
    "    print(\"Task 3 training mAP = \" + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774f2c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 11.6487 | Val mAP: 0.1148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 5.9244 | Val mAP: 0.2549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 0.2519 | Val mAP: 0.3816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 0.1962 | Val mAP: 0.5149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 0.1440 | Val mAP: 0.5821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Loss: 0.1044 | Val mAP: 0.6213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Loss: 0.0790 | Val mAP: 0.6446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [18:57<00:00, 10.07s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Loss: 0.0635 | Val mAP: 0.6585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [03:02<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Loss: 0.0475 | Val mAP: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [13:53<00:00,  7.38s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Loss: 0.0383 | Val mAP: 0.6573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Loss: 0.0330 | Val mAP: 0.6709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:38<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Loss: 0.0296 | Val mAP: 0.6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:40<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Loss: 0.0249 | Val mAP: 0.6628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:39<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Loss: 0.0232 | Val mAP: 0.6591\n",
      "Early stopping at epoch 14\n",
      "Task 3 training mAP = 0.9765348505386147\n"
     ]
    }
   ],
   "source": [
    "run3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8b523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
